{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering with K-Means and K-Medoids\n",
        "\n",
        "In this notebook, we will perform clustering on a synthetic dataset using K-Means and K-Medoids algorithms. We will determine the optimal number of clusters using the Elbow Method and Silhouette Score, and visualize the clustering results. Additionally, we will compare the clustering quality and visualize decision boundaries.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-PZoEE09MjpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "We begin by importing the necessary libraries for clustering, generating synthetic data, and visualizing the results.\n"
      ],
      "metadata": {
        "id": "0ivtAn8WMvbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngHUNqmUMgnc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.spatial.distance import cdist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate Synthetic Data\n",
        "\n",
        "We generate synthetic data (4-clusters) to simulate data points for clustering.\n"
      ],
      "metadata": {
        "id": "FfQMl0PBMzQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data for clustering\n",
        "X = np.vstack([\n",
        "    np.random.normal(loc=[2, 2], scale=0.5, size=(100, 2)),  # Cluster 1\n",
        "    np.random.normal(loc=[6, 6], scale=0.5, size=(100, 2)),  # Cluster 2\n",
        "    np.random.normal(loc=[10, 2], scale=0.5, size=(100, 2)),  # Cluster 3\n",
        "    np.random.normal(loc=[6, -3], scale=0.5, size=(100, 2))   # Cluster 4\n",
        "])\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], s=30, alpha=0.6)\n",
        "plt.title(\"Generated Data for Clustering\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3f2xyxogM1Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Determine Optimal K Using the Elbow Method\n",
        "\n",
        "The Elbow Method is used to find the optimal number of clusters by plotting the distortion (within-cluster sum of squared distances) as a function of K.\n"
      ],
      "metadata": {
        "id": "hkJi_Kj-M4ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distortions = []\n",
        "k_values = range(1, 11)\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    distortions.append(sum(np.min(cdist(X, kmeans.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
        "\n",
        "plt.plot(k_values, distortions, marker='o')\n",
        "plt.xlabel('Number of clusters, K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VGjmiKJyM3WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Determine Optimal K Using Silhouette Score\n",
        "\n",
        "The Silhouette Score helps to assess the quality of clustering by comparing the cohesion and separation of clusters. A higher Silhouette Score indicates better-defined clusters.\n"
      ],
      "metadata": {
        "id": "yqb6n_R4OtgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores = []\n",
        "k_values = range(2, 11)\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of clusters, K')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Optimal K')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2PsxRPD-OvXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Determine Optimal K Using CH (K) Index\n",
        "\n",
        "The CH(K) helps to assess the quality of clustering by comparing the cohesion and separation of clusters. A higher Silhouette Score indicates better-defined clusters."
      ],
      "metadata": {
        "id": "DqR51-qq0MPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ch_scores = []\n",
        "k_values = range(2, 11)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    ch_scores.append(calinski_harabasz_score(X, labels))\n",
        "\n",
        "plt.plot(k_values, ch_scores, marker='o')\n",
        "plt.xlabel('Number of clusters, K')\n",
        "plt.ylabel('Calinski-Harabasz Index')\n",
        "plt.title('Calinski-Harabasz Index for Optimal K')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fUymaSjF0BVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Gap Statistic Method for K-Selection\n",
        "Weâ€™ll define the Gap Statistic method. The steps are as follows:\n",
        "\n",
        "Perform K-Means clustering on the real data for each k (number of clusters).\n",
        "\n",
        "For each k, calculate the WCSS (Within-Cluster Sum of Squares).\n",
        "\n",
        "Generate random uniform data (as the null hypothesis) and perform K-Means on this data.\n",
        "\n",
        "Compute the gap statistic for each k and select the value of k that maximizes the gap."
      ],
      "metadata": {
        "id": "NuMsDdJZQlIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_wcss(X, k):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    return kmeans.inertia_\n",
        "\n",
        "def gap_statistic(X, k_max):\n",
        "    gaps = []\n",
        "    for k in range(1, k_max + 1):\n",
        "        # WCSS for real data\n",
        "        wcss_real = compute_wcss(X, k)\n",
        "\n",
        "        # Generate random uniform data (null hypothesis)\n",
        "        random_data = np.random.uniform(low=X.min(), high=X.max(), size=X.shape)\n",
        "\n",
        "        # WCSS for random data\n",
        "        wcss_random = compute_wcss(random_data, k)\n",
        "\n",
        "        # Compute the Gap statistic\n",
        "        gap = np.log(wcss_random) - np.log(wcss_real)\n",
        "        gaps.append(gap)\n",
        "\n",
        "    return gaps\n",
        "\n",
        "# Compute the Gap statistic for different values of k\n",
        "k_max = 10\n",
        "gaps = gap_statistic(X, k_max)\n",
        "\n",
        "# Plot the Gap statistic\n",
        "plt.plot(range(1, k_max + 1), gaps, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Gap Statistic')\n",
        "plt.title('Gap Statistic for K-Selection')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zxpn-uTIQpRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Apply K-Means Clustering\n",
        "\n",
        "After determining the optimal K=4 from the Elbow and Silhouette methods, we apply K-Means clustering and visualize the results with cluster centroids.\n"
      ],
      "metadata": {
        "id": "MhnbFK0AOyJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_k = 4  # Chosen based on Elbow and Silhouette methods\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=30, alpha=0.6)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DjUIG4vGOxNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Apply K-Medoids Clustering\n",
        "\n",
        "Similarly, we apply K-Medoids clustering (using the PAM method) and visualize the results with medoids.\n"
      ],
      "metadata": {
        "id": "cN4MUOKkO1_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of clusters\n",
        "optimal_k = 4\n",
        "\n",
        "# Randomly initialize medoids\n",
        "np.random.seed(42)\n",
        "medoids = X[np.random.choice(X.shape[0], optimal_k, replace=False)]\n",
        "prev_medoids = None\n",
        "\n",
        "# Repeat until medoids don't change\n",
        "while not np.array_equal(medoids, prev_medoids):\n",
        "    prev_medoids = medoids.copy()\n",
        "\n",
        "    # Compute distances from each point to each medoid\n",
        "    distances = cdist(X, medoids, 'euclidean')\n",
        "    labels = np.argmin(distances, axis=1)  # Assign points to nearest medoid\n",
        "\n",
        "    # Update medoids by choosing the point that minimizes the total distance within each cluster\n",
        "    for i in range(optimal_k):\n",
        "        cluster_points = X[labels == i]\n",
        "        if len(cluster_points) > 0:\n",
        "            medoid_idx = np.argmin(np.sum(cdist(cluster_points, cluster_points, 'euclidean'), axis=1))\n",
        "            medoids[i] = cluster_points[medoid_idx]\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.6)\n",
        "plt.scatter(medoids[:, 0], medoids[:, 1], c='red', marker='X', label='Medoids')\n",
        "plt.title(\"K-Medoids Clustering\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wv4YaIOh3bQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare K-Means and K-Medoids\n",
        "\n",
        "We compare the clustering quality metrics (inertia for K-Means and cost for K-Medoids).\n"
      ],
      "metadata": {
        "id": "masoFf45PCNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"K-Means Inertia: {kmeans.inertia_}\")\n",
        "kmedoids_cost = sum(np.min(cdist(X, medoids, 'euclidean'), axis=1))\n",
        "print(f\"K-Medoids Cost: {kmedoids_cost}\")"
      ],
      "metadata": {
        "id": "M0Rc6z6pPFfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluate Cluster Quality\n",
        "\n",
        "We evaluate the clustering performance of both algorithms using the Silhouette Score.\n"
      ],
      "metadata": {
        "id": "-CqbzvejPI4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_clustering(X, labels, method_name):\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    print(f\"Silhouette Score for {method_name}: {silhouette_avg:.4f}\")\n",
        "\n",
        "evaluate_clustering(X, kmeans_labels, \"K-Means\")\n",
        "evaluate_clustering(X, labels, \"K-Medoids\")\n"
      ],
      "metadata": {
        "id": "b1xkVFwoPKjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Visualizing Cluster Boundaries\n",
        "\n",
        "Finally, we visualize the decision boundaries of the clusters and display them on the plot. This helps to visualize how well the algorithm is separating the clusters.\n"
      ],
      "metadata": {
        "id": "YhZH3GCEPP_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_boundaries(X, labels, centers, title):\n",
        "    h = .02  # Step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = KMeans(n_clusters=optimal_k, random_state=42, n_init=10).fit_predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['yellow', 'blue']))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.6)\n",
        "    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', label='Centroids')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundaries(X, kmeans_labels, kmeans.cluster_centers_, \"K-Means Decision Boundaries\")\n",
        "plot_decision_boundaries(X, labels, medoids, \"K-Medoids Decision Boundaries\")\n"
      ],
      "metadata": {
        "id": "57FHbUxDQTCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we explored clustering with both K-Means and K-Medoids. We used the Elbow Method and Silhouette Score to determine the optimal number of clusters. We visualized the clustering results, compared the quality of clustering using inertia and cost metrics, and evaluated cluster quality using the Silhouette Score. Finally, we visualized the decision boundaries to show how well the algorithms separate the data into distinct clusters.\n"
      ],
      "metadata": {
        "id": "W2qd8l8LPVyw"
      }
    }
  ]
}