{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEno_OMZJYFF"
      },
      "source": [
        "# Case Study: Predicting Customer Purchase Behavior using Decision Tree with Grid Search\n",
        "\n",
        "In this case study, we use a Decision Tree classifier to predict customer purchases based on features like age, income, browsing time, and product category. We will then use Grid Search to tune hyperparameters and improve the model's performance."
      ],
      "id": "jEno_OMZJYFF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOeRkiLHJYFJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "id": "KOeRkiLHJYFJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "THbSk-hCJ4Yw"
      },
      "id": "THbSk-hCJ4Yw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate 1000 samples with random data for features\n",
        "n_samples = 5000\n",
        "\n",
        "# Age: Random integers between 18 and 65\n",
        "age = np.random.randint(18, 66, size=n_samples)\n",
        "\n",
        "# Income: Random integers between 30000 and 120000\n",
        "income = np.random.randint(30000, 120001, size=n_samples)\n",
        "\n",
        "# Browsing Time: Random float between 0 and 5 hours\n",
        "browsing_time = np.random.uniform(0, 5, size=n_samples)\n",
        "\n",
        "# Product Category: Randomly chosen from ['Electronics', 'Clothing', 'Groceries', 'Toys']\n",
        "product_category = np.random.choice(['Electronics', 'Clothing', 'Groceries', 'Toys'], size=n_samples)\n",
        "\n",
        "# Purchased: Target variable (0 = No, 1 = Yes), based on a simple rule: income and browsing time\n",
        "purchased = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])  # 30% purchase rate\n",
        "\n",
        "# Creating the DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Age': age,\n",
        "    'Income': income,\n",
        "    'Browsing Time': browsing_time,\n",
        "    'Product Category': product_category,\n",
        "    'Purchased': purchased\n",
        "})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jDzH6QS_J3ul"
      },
      "id": "jDzH6QS_J3ul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test split"
      ],
      "metadata": {
        "id": "jydlvZ2OKLTt"
      },
      "id": "jydlvZ2OKLTt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (Assume it's already cleaned)\n",
        "# Feature selection\n",
        "X = df[['Age', 'Income', 'Browsing Time', 'Product Category']]\n",
        "y = df['Purchased']\n",
        "\n",
        "# One-hot encode 'Product Category' since it's categorical\n",
        "X = pd.get_dummies(X, drop_first=True)  # This converts 'Product Category' into numerical columns\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "yTLofIy9KFuE"
      },
      "id": "yTLofIy9KFuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardize the features"
      ],
      "metadata": {
        "id": "uGc8WECqKOGR"
      },
      "id": "uGc8WECqKOGR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KLVgRxqLKRC0"
      },
      "id": "KLVgRxqLKRC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmxRHQXLJYFK"
      },
      "source": [
        "### Step 2: Initial Decision Tree Model\n",
        "\n",
        "Next, let's train a simple decision tree model and evaluate its performance without hyperparameter tuning."
      ],
      "id": "xmxRHQXLJYFK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW654ANOJYFL"
      },
      "source": [
        "# Initialize a Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(f'Accuracy without tuning: {accuracy_score(y_test, y_pred):.4f}')"
      ],
      "id": "OW654ANOJYFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_IOPIz1JYFL"
      },
      "source": [
        "### Step 3: Hyperparameter Tuning with Grid Search\n",
        "\n",
        "Now, let's perform hyperparameter tuning using Grid Search to find the best parameters for the Decision Tree model."
      ],
      "id": "M_IOPIz1JYFL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqp5YG2BJYFL"
      },
      "source": [
        "# Define parameter grid for Decision Tree\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 7, 10],\n",
        "    'min_samples_split': [5, 10, 15],\n",
        "    'min_samples_leaf': [1, 3, 5],\n",
        "    'max_features': [None, 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model with Grid Search\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and score from Grid Search\n",
        "print('Best Parameters: ', grid_search.best_params_)\n",
        "print('Best Cross-Validation Score: ', grid_search.best_score_)"
      ],
      "id": "Fqp5YG2BJYFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nPHAepdJYFM"
      },
      "source": [
        "### Step 4: Evaluate the Best Model\n",
        "\n",
        "Once the best hyperparameters are found, we use the best model to make predictions and evaluate its performance on the test set."
      ],
      "id": "0nPHAepdJYFM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVJk41c-JYFM"
      },
      "source": [
        "# Get the best model from grid search\n",
        "best_dt = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred_best = best_dt.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy of the best model\n",
        "print(f'Accuracy with Grid Search tuning: {accuracy_score(y_test, y_pred_best):.4f}')"
      ],
      "id": "vVJk41c-JYFM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZOWtVHzJYFM"
      },
      "source": [
        "### Step 5: Model Comparison\n",
        "\n",
        "Finally, we compare the accuracy of the initial model (without tuning) and the tuned model (with Grid Search)."
      ],
      "id": "BZOWtVHzJYFM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDlZek_JJYFN"
      },
      "source": [
        "# Comparison of accuracy\n",
        "initial_accuracy = accuracy_score(y_test, y_pred)\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f'Accuracy without tuning: {initial_accuracy:.4f}')\n",
        "print(f'Accuracy with tuning (Grid Search): {tuned_accuracy:.4f}')"
      ],
      "id": "NDlZek_JJYFN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDlBayGRJYFN"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Grid Search allows for systematic hyperparameter tuning and typically results in a more accurate model. By adjusting the hyperparameters of the Decision Tree, such as `max_depth`, `min_samples_split`, and `min_samples_leaf`, we can improve the model's performance and avoid overfitting or underfitting."
      ],
      "id": "TDlBayGRJYFN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x6X0pKfJYFN"
      },
      "source": [
        "### Additional Notes\n",
        "\n",
        "- **GridSearchCV** performs exhaustive search over the specified hyperparameters and uses cross-validation to evaluate each combination.\n",
        "- You can further refine the search by using **RandomizedSearchCV** if the search space is too large."
      ],
      "id": "4x6X0pKfJYFN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}